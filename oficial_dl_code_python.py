# -*- coding: utf-8 -*-
"""Diff in Diff analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZy9Lk7gzqJv7UALfsiYqNSVdGFQOfpF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import os
import yfinance as yf
import statsmodels.api as sm
import seaborn as sns
from pathlib import Path
!pip install linearmodels

# === A. Read and Join BIS database ===



import glob, pandas as pd

# 1)
files = sorted(glob.glob("bis_dp_search_export_*.xlsx"))
assert len(files) > 0, "No files bis_dp_search_export_*.xlsx"

def read_bis_policy(path: str) -> pd.DataFrame:
    # sheet with needed obs
    df = pd.read_excel(path, sheet_name="timeseries observations")
    # standarize names
    df = df.rename(columns={
        "TIME_PERIOD:Period": "date",
        "OBS_VALUE:Value": "rate",
        "REF_AREA:Reference area": "ref_area"
    })
    # needed columns
    df = df[["date","rate","ref_area"]].copy()

    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df["rate"] = pd.to_numeric(df["rate"], errors="coerce")
    df = df.dropna(subset=["date","rate"])
    # Country Codes
    code = df["ref_area"].astype(str).str.split(":", n=1, expand=True)[0].str.strip()
    # Tags
    map_country = {"XM":"EA","US":"US","GB":"UK","JP":"JP","AU":"AU"}
    df["country"] = code.map(map_country).fillna(code)  # por si apareciera otro código
    df = df.drop(columns=["ref_area"])
    df["_source"] = path
    return df

frames = [read_bis_policy(p) for p in files]
rates = pd.concat(frames, ignore_index=True).sort_values(["country","date"]).reset_index(drop=True)

print("Founded countries:", rates["country"].unique().tolist())
print(rates.groupby("country")["date"].agg(["min","max","count"]))
rates.head(20)

# === 1) EVENTS RatesBIS ===
import numpy as np
import pandas as pd

# DataFrame 'rates'(date, rate, country, _source)
rates = rates.sort_values(['country','date']).copy()

# daily rate differences
rates['drate'] = rates.groupby('country')['rate'].diff()

# event if |Δrate| >= 50 bps
RATE_EVENT_BPS = 0.50
rates['event_day'] = (rates['drate'].abs() >= RATE_EVENT_BPS).astype(int)

# week attached to monday
rates['week'] = rates['date'] - pd.to_timedelta(rates['date'].dt.weekday, unit='D')

# at least 1 event that week
week_event = (rates.groupby(['country','week'])['event_day']
                   .max().reset_index().rename(columns={'event_day':'event_week'}))

print("weeks with event (sample):")
print(week_event.query("event_week==1").head(8))

#first week with big cut  (Δrate acumulado <= -50 bps) en 2020H1
H1_START, H1_END = "2020-01-01", "2020-06-30"
mask_h1 = (rates['date'] >= H1_START) & (rates['date'] <= H1_END)


tmp = (rates.loc[mask_h1, ['country','date','drate']]
           .assign(neg_cut=lambda d: d['drate'].where(d['drate'] < 0, 0.0))
           .sort_values(['country','date']))
tmp['cum_cut'] = tmp.groupby('country')['neg_cut'].cumsum()

first_hit = (tmp.loc[tmp['cum_cut'] <= -RATE_EVENT_BPS]
                .groupby('country')['date'].min()
                .rename('first_cut')).reset_index()


first_hit['first_cut_week'] = first_hit['first_cut'].dt.to_period('W-FRI').dt.end_time.dt.normalize()

first_cut = first_hit

# cumulated cut H1 (Sum Δrate)
cut_sum = (rates.loc[mask_h1].groupby('country')['drate']
                 .sum().rename('cut_sum_h1')).reset_index()

print("\nFirst Week with cut>=50bps (2020H1):")
print(first_cut)
print("\nCumulated cut 2020H1 (bps):")
print(cut_sum)

# guardar intermedios
import os
os.makedirs("outputs", exist_ok=True)
rates.to_csv("outputs/rates_clean.csv", index=False)
week_event.to_csv("outputs/week_event.csv", index=False)
first_cut.to_csv("outputs/first_cut.csv", index=False)
cut_sum.to_csv("outputs/cut_sum.csv", index=False)

START = "2019-10-01"
END   = "2021-12-31"

# Country with representative index
TICKERS = {
    "EA": "^STOXX50E",   # EuroStoxx 50 (Euro Area)
    "US": "^GSPC",       # S&P 500
    "UK": "^FTSE",       # FTSE 100
    "JP": "^N225",       # Nikkei 225
    "AU": "^AXJO",       # ASX 200
}

# Daily data as close,open,etc
def download_index_panel(tickers: dict, start: str, end: str) -> pd.DataFrame:


    frames = []
    for c, t in tickers.items():
        print(f"[YF] {c}: {t}")
        d = yf.download(t, start=start, end=end, progress=False, auto_adjust=False)
        if d is None or d.empty:
            raise ValueError(f"No data {t}")


        if isinstance(d.columns, pd.MultiIndex):
            d.columns = ['_'.join([str(x) for x in col if str(x)!='']).strip() for col in d.columns]
        else:
            d.columns = [str(x) for x in d.columns]

        #  Close/High/Low de forma flexible
        def pick(colname, avoid_adj=False):
            tmp = [c for c in d.columns if colname in c.lower()]
            if avoid_adj:
                tmp = [c for c in tmp if 'adj' not in c.lower()]

            for exact in [colname.capitalize(), colname.upper(), colname.lower()]:
                if exact in d.columns: return exact
            if tmp: return tmp[0]
            return None

        close_col = pick('close', avoid_adj=True) or pick('adj close')
        high_col  = pick('high')
        low_col   = pick('low')

        if not all([close_col, high_col, low_col]):
            raise ValueError(f"⚠️ No found columns Close/High/Low for {t}. "
                             f"Cols dispo: {list(d.columns)[:8]}...")

        df = d[[close_col, high_col, low_col]].copy()
        df.columns = ["Close","High","Low"]
        df = df.dropna(how="any")
        df["country"] = c
        frames.append(df.reset_index().rename(columns={"Date":"date"}))

    panel = pd.concat(frames, ignore_index=True)
    return panel
px = download_index_panel(TICKERS, START, END)
px = px.sort_values(["country","date"]).copy()

px["log_close"] = np.log(px["Close"])
px["ret"] = px.groupby("country")["log_close"].diff()
px["rv_day"] = px["ret"].pow(2)
px["parkinson_day"] = (np.log(px["High"] / px["Low"])**2) / (4*np.log(2))

print(px.head())
print(px.groupby("country")["ret"].apply(lambda s: s.isna().sum()).to_frame("na_ret_rows"))

# 1) Check if dates are correct
assert px.groupby("country")["date"].is_monotonic_increasing.all(), "Dates no ordered"

max_err = (px["rv_day"] - px["ret"]**2).abs().max()
print("max |rv_day - ret^2| =", float(max_err))

# High >= Low  no nuls
bad_hl = ((px["High"] < px["Low"]) | px["High"].isna() | px["Low"].isna()).sum()
print("filas con High<Low o NaN en HL:", int(bad_hl))

# 4) Distribution returns by country
print(px.groupby("country")["ret"].describe()[["count","mean","std","min","max"]])

# Weekly agregated

# Summarize weekly data
def weekly_aggr(g):
    g = g.set_index("date").sort_index()
    out = pd.DataFrame({
        "rv_week": g["rv_day"].resample("W-FRI").sum(),          # realized variance
        "parkinson_week": g["parkinson_day"].resample("W-FRI").mean(),  # Parkinson
        "ret_week": g["ret"].resample("W-FRI").sum(),            # Weekly return
    })
    return out

wk = (
    px.set_index("date")
      .groupby("country")
      .resample("W-FRI")
      .agg(
          rv_week=("rv_day", "sum"),
          parkinson_week=("parkinson_day", "mean"),
          ret_week=("ret", "sum"),
      )
      .reset_index()
      .rename(columns={"date": "week"})
)

# outcomes logarítmicos
wk["Y_vol"] = np.log(wk["rv_week"])
wk["Y_vol_alt"] = np.log(wk["parkinson_week"])

# quitar semanas sin info
wk = wk.dropna(subset=["Y_vol", "Y_vol_alt"]).copy()

wk.to_csv("outputs/weekly_market.csv", index=False)
print(" weekly_market.csv saved. Sample:")
print(wk.head(8))
print("\nObs by country:")
print(wk.groupby("country")["week"].count())

# Create subplots for each country

g = sns.displot(
    data=px, x='ret',
    col='country', col_wrap=3,
    kind='hist', kde=True,
    height=2.2, aspect=2.2,
    facet_kws={'sharex': True, 'sharey': False}
)

#Join BIS and market info
import pandas as pd
import numpy as np
import os

# Read previous info
wk         = pd.read_csv("outputs/weekly_market.csv", parse_dates=["week"])
week_event = pd.read_csv("outputs/week_event.csv", parse_dates=["week"])
first_cut  = pd.read_csv("outputs/first_cut.csv", parse_dates=["first_cut","first_cut_week"])
cut_sum    = pd.read_csv("outputs/cut_sum.csv")

# Join in a panel
panel = (wk
    .merge(week_event, on=["country","week"], how="left")
    .merge(first_cut[["country","first_cut_week"]], on="country", how="left")
    .merge(cut_sum[["country","cut_sum_h1"]], on="country", how="left")
)


panel["event_week"] = panel["event_week"].fillna(0).astype(int)

# Define 'post' (1 after first cut)
panel["post"] = ((panel["week"] >= panel["first_cut_week"]) & panel["first_cut_week"].notna()).astype(int)

#  'treated' (CUT ≥ 0.5 bps)
TREATED_CUT_SUM = -0.5
panel["treated"] = (panel["cut_sum_h1"] <= TREATED_CUT_SUM).astype(int)

#  DiD
panel["did"] = panel["post"] * panel["treated"]


# Controls
panel["abs_ret_week"] = panel["ret_week"].abs()                     # Returns
panel["lag_Y_vol"] = panel.groupby("country")["Y_vol"].shift(1)    # Volatility persistence

os.makedirs("outputs", exist_ok=True)
panel.to_csv("outputs/panel_weekly.csv", index=False)

print("✅ Combined Panel saved : outputs/panel_weekly.csv")
print(panel.head(10))
print("\Treated count by country :")
print(panel.groupby("country")[["treated","post"]].max())

def summarize_by_country(df):
    g = df.groupby("country")
    out = g.agg(
        first_cut_week=("first_cut_week","first"),
        cut_sum_h1=("cut_sum_h1","first"),
        treated=("treated","max"),
        post_start=("week", lambda s: df.loc[(df["country"]==s.iloc[0]) & (df["post"]==1),"week"].min()),
        post_weeks=("post","sum"),
        did_weeks=("did","sum"),
        any_event=("event_week","sum"),
    ).reset_index()
    return out

chk = summarize_by_country(panel)
print("=== summarize_by_country ===")
print(chk)

vix = yf.download("^VIX",
                  start=panel["week"].min(),
                  end=panel["week"].max() + pd.Timedelta(days=7),
                  progress=False, auto_adjust=False)


if isinstance(vix.columns, pd.MultiIndex):
    vix.columns = ['_'.join([str(x) for x in col if str(x)!='']).strip() for col in vix.columns]
else:
    vix.columns = [str(c) for c in vix.columns]

#Close columns
close_col = None
for cand in ["Close","Adj Close","Close_","AdjClose","close","adj close","Adj Close"]:
    if cand in vix.columns:
        close_col = cand
        break
if close_col is None:

    cands = [c for c in vix.columns if "close" in c.lower()]
    if cands:
        close_col = cands[0]
    else:
        raise ValueError(f"No encuentro columna Close en VIX. Cols: {vix.columns.tolist()[:8]}")

vix = vix[[close_col]].rename(columns={close_col: "VIX"})

# 3)  Weekly adjusted with friday + log
vix.index = pd.to_datetime(vix.index)
vix_w = vix.resample("W-FRI").mean().reset_index()
vix_w = vix_w.rename(columns={"Date":"week", "date":"week"})
vix_w["week"] = pd.to_datetime(vix_w["week"])
vix_w["log_VIX"] = np.log(vix_w["VIX"])

if isinstance(vix_w.columns, pd.MultiIndex):
    vix_w.columns = vix_w.columns.get_level_values(0)

panel["week"] = pd.to_datetime(panel["week"])

# 6) MERGE CLEAN
panel2 = panel.merge(vix_w[["week","log_VIX"]], on="week", how="left")

# Additional controls
panel2["abs_ret_week"] = panel2["ret_week"].abs()
panel2["lag_Y_vol"] = panel2.groupby("country")["Y_vol"].shift(1)

print(panel2[["country","week","Y_vol","ret_week","log_VIX","post","treated","did"]].head(8))

# Panel with VIX
df = panel2.dropna(subset=["Y_vol","ret_week","log_VIX","lag_Y_vol"]).copy()

# Centered control
df["ret_week_c"] = df["ret_week"] - df["ret_week"].mean()
df["abs_ret_week_c"] = df["abs_ret_week"] - df["abs_ret_week"].mean()
df["log_VIX_c"] = df["log_VIX"] - df["log_VIX"].mean()
df["lag_Y_vol_c"] = df["lag_Y_vol"] - df["lag_Y_vol"].mean()

# Fixed effects with dummies
d_country = pd.get_dummies(df["country"], drop_first=True, prefix="C")
d_week    = pd.get_dummies(df["week"], drop_first=True, prefix="W")

# Matrix regressors
X = pd.concat([
    df[["did","ret_week_c","abs_ret_week_c","log_VIX_c","lag_Y_vol_c"]].astype(float),
    d_country, d_week
], axis=1)
X = sm.add_constant(X)

y = df["Y_vol"].values

"""# Clean FE regression with PanelOLS (no dummy explosion)"""

# --- 0) Inspect what's inside panel2
print("Columns in panel2:\n", sorted(panel2.columns.tolist())[:20], "...")

# --- 1) Clean types & sort
if pd.api.types.is_period_dtype(panel2['week']):
    panel2['week'] = panel2['week'].dt.start_time
panel2 = panel2.sort_values(['country','week']).copy()

# --- 2) Create centered controls (based on your columns list)
# present: 'ret_week', 'abs_ret_week', 'lag_Y_vol', 'log_VIX' (optional)
panel2['ret_week_c']     = panel2['ret_week'] - panel2['ret_week'].mean()
panel2['abs_ret_week_c'] = panel2['abs_ret_week'] - panel2['abs_ret_week'].mean()
panel2['lag_Y_vol_c']    = panel2['lag_Y_vol'] - panel2['lag_Y_vol'].mean()
if 'log_VIX' in panel2:
    panel2['log_VIX_c'] = panel2['log_VIX'] - panel2['log_VIX'].mean()

# --- 3) Choose the treatment variable you already have
# you have both 'did' and 'event_week'; we’ll use 'did'
treat = 'did'

# --- 4) Baseline FE DiD: country FE + week FE (no VIX because it collinear with week FE)
from linearmodels.panel import PanelOLS
import statsmodels.api as sm

use = panel2.dropna(subset=['Y_vol', treat, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c']).copy()
use = use.set_index(['country','week']).sort_index()

X = sm.add_constant(use[[treat, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c']])
mod = PanelOLS(use['Y_vol'], X, entity_effects=True, time_effects=True)

# Driscoll–Kraay SEs (good with 5 countries + cross-sectional dependence)
res = mod.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)
print(res.summary)



"""**Explanation of results:**
The coefficient associated with did (−0.0058) is not statistically significant, indicating that large monetary policy changes of 50 basis points or more did not lead to an abnormal increase or decrease in weekly stock market volatility once we control for market conditions and persistence.

Instead, volatility appears to respond primarily to market turbulence (captured by the absolute weekly return) and exhibits persistence across time. These findings suggest that, during the COVID-19 crisis, volatility dynamics were dominated by global uncertainty rather than idiosyncratic policy rate changes.

# (A) Dynamic event-study

**To see if volatility rises before or after a policy move.**
"""

# 1) Build event-time dummies (leads/lags)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from linearmodels.panel import PanelOLS

# Window size
LEADS, LAGS = 4, 6   # change if you want wider/narrower bands

def build_event_time(df, event_col='did', leads=LEADS, lags=LAGS):
    """Create τ (relative-week) and dummies E_{τ} with τ∈[-leads, +lags].
       Reference/baseline will be τ = -1 (the week before the event)."""
    g = df.sort_values(['country','week']).copy()
    # ensure week is datetime
    if pd.api.types.is_period_dtype(g['week']):
        g['week'] = g['week'].dt.start_time

    g['event_time'] = np.nan
    for c, sub in g.groupby('country', sort=False):
        ev_weeks = sub.loc[sub[event_col] == 1, 'week'].values
        if ev_weeks.size == 0:
            continue
        weeks = sub['week'].values
        # distance to nearest event week (in integer weeks)
        dist = [int((w - ev_weeks[np.argmin(np.abs(w - ev_weeks))]) / np.timedelta64(1, 'W')) for w in weeks]
        g.loc[sub.index, 'event_time'] = dist

    # Generate dummies
    for tau in range(-leads, lags + 1):
        g[f'E_{tau:+d}'] = (g['event_time'] == tau).astype(int)

    return g

es = build_event_time(panel2, event_col='did', leads=LEADS, lags=LAGS)
# We'll omit τ = -1 as the baseline category
base = 'E_-1'
es_cols = [c for c in es.columns if c.startswith('E_') and c != base]
print("Event-study dummies:", es_cols[:5], "… total:", len(es_cols))

# 2) Estimate event-study with country & week FE (fixed version)

# Restrict to ±LEADS/LAGS window and drop rows without event_time
es_win = es[es['event_time'].between(-LEADS, LAGS)].copy()
use_es = es_win.dropna(subset=['Y_vol']).set_index(['country','week']).sort_index()

# Build regressors: all event-time dummies except τ = -1
X_es = use_es[[c for c in es.columns if c.startswith('E_') and c != 'E_-1']].copy()

# Drop any τ columns that are all zeros or constant
nonzero_cols = [c for c in X_es.columns if X_es[c].sum() > 0 and X_es[c].var() > 0]
X_es = X_es[nonzero_cols]

from linearmodels.panel import PanelOLS
mod_es = PanelOLS(use_es['Y_vol'], X_es, entity_effects=True, time_effects=True)

# Allow collinear columns to be dropped automatically
res_es = mod_es.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)
print(res_es.summary)

"""The dynamic event-study confirms that stock-market volatility sharply increases in the week of large monetary policy changes. The estimated coefficient for τ = 0 is around 1.15 (p < 0.01), implying that volatility more than doubles relative to normal weeks. In contrast, pre-event coefficients (τ = −4 to −2) are small and statistically insignificant, supporting the parallel-trend assumption. This pattern suggests that markets did not anticipate these abrupt policy actions, which produced a temporary volatility surge during announcement weeks."""

# 3) Plot event-study coefficients
coefs = res_es.params
ses   = res_es.std_errors
taus  = np.array([int(c.split('_')[1]) for c in coefs.index])
order = np.argsort(taus)

plt.figure(figsize=(7,4))
plt.axhline(0, lw=1, alpha=0.7)
plt.axvline(-1, lw=1, ls='--', alpha=0.7)
plt.plot(taus[order], coefs.values[order], marker='o')
z = 1.96
plt.fill_between(taus[order],
                 coefs.values[order] - z*ses.values[order],
                 coefs.values[order] + z*ses.values[order],
                 alpha=0.2)
plt.title('Event Study: log-volatility around ≥50 bps policy moves')
plt.xlabel('Weeks relative to event (τ)')
plt.ylabel('β(τ) vs τ = −1')
plt.tight_layout()
plt.show()

"""The image plots the dynamic response of weekly stock-market volatility to large (≥ 50 bps) policy-rate changes.
The pre-event coefficients are statistically indistinguishable from zero, validating the parallel-trend assumption.
In contrast, the coefficient for the event week (τ = 0) is positive and significant, indicating that volatility rises markedly during policy-change weeks.
This pattern suggests that abrupt monetary interventions during the COVID-19 crisis generated short-term uncertainty surges in financial markets, while no evidence of anticipatory effects is found.
"""

# parallel-trends visual (FE-demeaned mean)
tmp = es.dropna(subset=['Y_vol']).copy()

# FE-demean proxy: remove country & week means
tmp['Y_dm'] = (tmp['Y_vol']
               - tmp.groupby('country')['Y_vol'].transform('mean')
               - tmp.groupby('week')['Y_vol'].transform('mean')
               + tmp['Y_vol'].mean())

# Compute average FE-demeaned log-volatility by event-time τ
pt = (tmp[tmp['event_time'].between(-LEADS, LAGS)]
      .groupby('event_time')['Y_dm'].mean())

# Plot
plt.figure(figsize=(7,4))
plt.axhline(0, lw=1, alpha=0.7)
plt.plot(pt.index, pt.values, marker='o')
plt.title('FE-demeaned mean of log-volatility by τ')
plt.xlabel('Weeks relative to event (τ)')
plt.ylabel('Mean (demeaned)')
plt.tight_layout()
plt.show()

"""The Figure plots the average fixed-effects-demeaned log-volatility by weeks relative to large monetary policy changes. The line remains approximately flat before τ = 0, supporting the parallel-trend assumption underlying the DiD identification. This pattern indicates that markets exhibited no systematic changes in volatility prior to policy actions, while volatility dynamics around the event were short-lived.

# Robustness checks
Runnning the same baseline DiD including global risk proxy (log_VIX_c)
"""

if 'log_VIX_c' in panel2:
    uv = panel2.dropna(subset=['Y_vol','did','ret_week_c','abs_ret_week_c','lag_Y_vol_c','log_VIX_c']).copy()
    uv = uv.set_index(['country','week']).sort_index()
    Xv = sm.add_constant(uv[['did','ret_week_c','abs_ret_week_c','lag_Y_vol_c','log_VIX_c']])
    mod_v = PanelOLS(uv['Y_vol'], Xv, entity_effects=True, time_effects=False)
    res_v = mod_v.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)
    print(res_v.summary)

# =========  Imports & assumptions =========
import pandas as pd, numpy as np, glob, re
from linearmodels.panel import PanelOLS
import statsmodels.api as sm

# panel2 must already exist and contain: country (AU, EA, JP, UK, US), week (datetime), Y_vol, ret_week_c, abs_ret_week_c, lag_Y_vol_c

# ========= 1) Load BIS Excel files from "timeseries observations" =========
# Change this to where your BIS files are
bis_path = '/content'          # or '/content' on Colab
bis_glob = f"{bis_path}/bis_dp_search_export_*.xlsx"

frames = []
for f in glob.glob(bis_glob):
    try:
        df = pd.read_excel(f, sheet_name="timeseries observations", header=0)
        df['source_file'] = f.split('/')[-1]
        frames.append(df)
        print(f"✓ Loaded {df.shape[0]:>6} rows from {df['source_file'].iloc[0]}")
    except Exception as e:
        print(f" {f}: {e}")

if not frames:
    raise RuntimeError("No BIS files loaded. Check bis_path/glob.")

bis_raw = pd.concat(frames, ignore_index=True)
print("BIS columns (first 12):", list(bis_raw.columns)[:12])

# ========= 2) Detect columns & tidy; compute Δ within each timeseries =========
def find_col(cols, patterns):
    cols = [str(c) for c in cols]
    for pat in patterns:
        for c in cols:
            if re.search(pat, c, flags=re.I):
                return c
    return None

c_country = find_col(bis_raw.columns, [r'^REF_AREA', r'Reference area'])
c_date    = find_col(bis_raw.columns, [r'^TIME_PERIOD', r'Time period', r':Period$'])
c_value   = find_col(bis_raw.columns, [r'^OBS_VALUE', r'Observation value', r':Value$'])
c_key     = find_col(bis_raw.columns, [r'^KEY:?\s*Timeseries Key', r'Timeseries Key', r'\bKEY\b'])

if any(x is None for x in [c_country, c_date, c_value, c_key]):
    raise ValueError("Could not detect BIS columns:", c_country, c_date, c_value, c_key)

bis_full = (bis_raw[[c_country, c_date, c_value, c_key]]
            .rename(columns={c_country:'country_raw', c_date:'date',
                             c_value:'policy_rate', c_key:'ts_key'})
            .copy())

bis_full['date'] = pd.to_datetime(bis_full['date'], errors='coerce')
bis_full['policy_rate'] = pd.to_numeric(bis_full['policy_rate'], errors='coerce')
bis_full = bis_full.dropna(subset=['country_raw','date','policy_rate','ts_key'])

# 'US:United States' -> 'US'
bis_full['country'] = bis_full['country_raw'].astype(str).str.split(':').str[0].str.strip()

# Δ within (country, timeseries)
bis_full = bis_full.sort_values(['country','ts_key','date'])
bis_full['delta_rate'] = bis_full.groupby(['country','ts_key'])['policy_rate'].diff()

# Collapse to country-day max |Δ|
daily = (bis_full.groupby(['country','date'], as_index=False)
         .agg(max_abs_delta=('delta_rate', lambda s: s.abs().max(skipna=True))))

# ========= 3) Align to panel weeks (Friday end), build weekly events =========
# Infer anchor from panel2 (should be Friday=4)
anchor_wd = int(pd.to_datetime(panel2['week']).dt.weekday.mode()[0])
weekday_name = {0:'MON',1:'TUE',2:'WED',3:'THU',4:'FRI',5:'SAT',6:'SUN'}[anchor_wd]
print("Detected panel week anchor:", weekday_name)

# Restrict to panel2 date span
dmin, dmax = pd.to_datetime(panel2['week']).min(), pd.to_datetime(panel2['week']).max()
daily = daily[(daily['date'] >= dmin) & (daily['date'] <= dmax)].copy()

# Calculate the number of days to add to reach the end of the week (Friday=4)
# Ensure this calculation is done element-wise and the result is a Series of integers
days_to_friday = (4 - daily['date'].dt.weekday)

# Force each daily date to the corresponding Friday (week end)
# Use pd.to_timedelta with the Series of days to add
daily['week'] = (daily['date'] + pd.to_timedelta(days_to_friday, unit='D')).dt.floor('D')

# Weekly max and event flags
events = (daily.groupby(['country','week'], as_index=False)
          .agg(max_abs_delta=('max_abs_delta','max')))

for thr, name in [(0.25,'event_wk25'), (0.50,'event_wk50'), (0.75,'event_wk75')]:
    events[name] = (events['max_abs_delta'] >= thr).astype(int)

# Map BIS codes to your panel codes
events['country'] = events['country'].replace({'GB':'UK','XM':'EA'})

# Diagnostic: check event counts
diag = (events.groupby('country')[['event_wk25','event_wk50','event_wk75']]
        .sum().T.rename(index={'event_wk25':'n_25bps','event_wk50':'n_50bps','event_wk75':'n_75bps'}))
print("\nEvent counts by country (before merge):")
display(diag)

# ========= 4) Normalize timestamps and merge =========
panel2 = panel2.copy()
events  = events.copy()

panel2['week_key'] = pd.to_datetime(panel2['week']).dt.normalize()
events['week_key'] = pd.to_datetime(events['week']).dt.normalize()

# Drop previous flags if any, then merge on ['country','week_key']
panel2 = panel2.drop(columns=[c for c in ['event_wk25','event_wk50','event_wk75'] if c in panel2.columns],
                     errors='ignore')
panel2 = panel2.merge(
    events[['country','week_key','event_wk25','event_wk50','event_wk75']],
    on=['country','week_key'], how='left'
)

for c in ['event_wk25','event_wk50','event_wk75']:
    panel2[c] = panel2[c].fillna(0).astype(int)

# Diagnostics
diag = (panel2.groupby('country')[['event_wk25','event_wk50','event_wk75']]
        .sum().astype(int)
        .rename(columns={'event_wk25':'n_25bps','event_wk50':'n_50bps','event_wk75':'n_75bps'}))
print("Event counts by country:")
display(diag)

# (Optional) sanity check around Mar-2020
chk = panel2.query("country in ['US','UK','EA','AU'] and week_key.between('2020-02-14','2020-04-10')")
display(chk[['country','week_key','event_wk25','event_wk50','event_wk75']]
        .sort_values(['country','week_key']).head(40))

# ========= 5A) Baseline DiD with week FE (25 & 75 bps) =========
results = []
for thr in ['event_wk25','event_wk75']:
    ut = panel2.dropna(subset=['Y_vol', thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c']).copy()
    ut = ut.set_index(['country','week_key']).sort_index()

    X = sm.add_constant(
        ut[[thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c']].rename(columns={thr:'did'})
    )
    mod = PanelOLS(ut['Y_vol'], X, entity_effects=True, time_effects=True)
    res = mod.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)

    print("\n=== Threshold:", thr.replace('event_wk',''), "bps — with week FE ===")
    print(res.summary)

    results.append({
        'threshold': thr.replace('event_wk',''),
        'spec': 'week FE',
        'beta_did': res.params['did'],
        'se_did': res.std_errors['did'],
        'pval_did': res.pvalues['did'],
        'nobs': int(res.nobs)
    })

res_table = pd.DataFrame(results)
display(res_table)

# ========= 5B) With VIX (no week FE to avoid collinearity) =========
if 'log_VIX_c' in panel2.columns:
    results_vix = []
    for thr in ['event_wk25','event_wk75']:
        uv = panel2.dropna(subset=['Y_vol', thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c', 'log_VIX_c']).copy()
        uv = uv.set_index(['country','week_key']).sort_index()

        Xv = sm.add_constant(
            uv[[thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c', 'log_VIX_c']].rename(columns={thr:'did'})
        )
        modv = PanelOLS(uv['Y_vol'], Xv, entity_effects=True, time_effects=False)
        resv = modv.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)

        print("\n=== Threshold:", thr.replace('event_wk',''), "bps — with VIX, no week FE ===")
        print(resv.summary)

        results_vix.append({
            'threshold': thr.replace('event_wk',''),
            'spec': 'VIX, no week FE',
            'beta_did': resv.params['did'],
            'se_did': resv.std_errors['did'],
            'pval_did': resv.pvalues['did'],
            'nobs': int(resv.nobs)
        })
    res_table_vix = pd.DataFrame(results_vix)
    display(res_table_vix)

from linearmodels.panel import PanelOLS
import statsmodels.api as sm
import pandas as pd

results = []

for thr in ['event_wk25','event_wk75']:
    ut = panel2.dropna(subset=['Y_vol', thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c']).copy()
    ut = ut.set_index(['country','week_key']).sort_index()

    X = sm.add_constant(
        ut[[thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c']].rename(columns={thr:'did'})
    )

    mod = PanelOLS(ut['Y_vol'], X, entity_effects=True, time_effects=True)
    res = mod.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)

    print("\n=== Threshold:", thr.replace('event_wk',''), "bps — with week FE ===")
    print(res.summary)

    results.append({
        'threshold': thr.replace('event_wk',''),
        'spec': 'week FE',
        'beta_did': res.params['did'],
        'se_did': res.std_errors['did'],
        'pval_did': res.pvalues['did'],
        'nobs': int(res.nobs)
    })

res_table = pd.DataFrame(results)
display(res_table)

if 'log_VIX_c' in panel2.columns:
    results_vix = []
    for thr in ['event_wk25','event_wk75']:
        uv = panel2.dropna(subset=['Y_vol', thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c', 'log_VIX_c']).copy()
        uv = uv.set_index(['country','week_key']).sort_index()

        Xv = sm.add_constant(
            uv[[thr, 'ret_week_c', 'abs_ret_week_c', 'lag_Y_vol_c', 'log_VIX_c']].rename(columns={thr:'did'})
        )

        modv = PanelOLS(uv['Y_vol'], Xv, entity_effects=True, time_effects=False)
        resv = modv.fit(cov_type='kernel', kernel='bartlett', bandwidth=4)

        print("\n=== Threshold:", thr.replace('event_wk',''), "bps — with VIX, no week FE ===")
        print(resv.summary)

        results_vix.append({
            'threshold': thr.replace('event_wk',''),
            'spec': 'VIX, no week FE',
            'beta_did': resv.params['did'],
            'se_did': resv.std_errors['did'],
            'pval_did': resv.pvalues['did'],
            'nobs': int(resv.nobs)
        })

    res_table_vix = pd.DataFrame(results_vix)
    display(res_table_vix)

